# Journal de D√©ploiement - Enterprise Security Stack sur Kubernetes

> Documentation compl√®te et chronologique du projet de stack de cybers√©curit√© d'entreprise

**Auteur** : Z3ROX
**Date de cr√©ation** : Novembre 2025
**Objectif** : D√©ployer une stack de s√©curit√© production-ready √©quivalente aux solutions commerciales (CrowdStrike, Splunk, Okta, etc.)

---

## üìã Table des Mati√®res

1. [Vue d'Ensemble du Projet](#vue-densemble-du-projet)
2. [Architecture Globale](#architecture-globale)
3. [Installation Chronologique des Composants](#installation-chronologique-des-composants)
4. [Probl√®mes Rencontr√©s et Solutions](#probl√®mes-rencontr√©s-et-solutions)
5. [Configuration R√©seau et Ingress](#configuration-r√©seau-et-ingress)
6. [√âtat Actuel du Projet](#√©tat-actuel-du-projet)
7. [Acc√®s et Credentials](#acc√®s-et-credentials)
8. [Scripts et Outils Cr√©√©s](#scripts-et-outils-cr√©√©s)

---

## üéØ Vue d'Ensemble du Projet

### Objectif

D√©monstrer comment construire une **stack de cybers√©curit√© d'entreprise moderne** sur Kubernetes, √©quivalente aux solutions commerciales utilis√©es dans les grandes organisations.

### √âquivalences Commerciales

| Composant D√©ploy√© | √âquivalent Commercial | R√¥le |
|-------------------|----------------------|------|
| Keycloak | Okta, Azure AD | IAM / SSO |
| HashiCorp Vault | AWS Secrets Manager, CyberArk | Secrets Management |
| ELK Stack | Splunk, QRadar | SIEM |
| Falco | CrowdStrike Falcon | Runtime Security |
| Wazuh | SentinelOne | EDR/XDR |
| Trivy | Snyk, Aqua | Vulnerability Scanning |
| OPA Gatekeeper | Prisma Cloud | Policy Enforcement |

### Infrastructure de Base

- **Cluster Kubernetes** : Kind (Kubernetes in Docker)
- **Namespaces** :
  - `security-iam` : IAM et Secrets Management
  - `security-siem` : Observabilit√© et SIEM
  - `security-detection` : Runtime Security
  - `ingress-nginx` : Ingress Controller
  - `metallb-system` : Load Balancer
  - `cert-manager` : Gestion des certificats TLS

---

## üèóÔ∏è Architecture Globale

### Sch√©ma d'Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    COUCHE D'ACC√àS (Ingress)                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  MetalLB (Load Balancer) ‚Üí NGINX Ingress Controller     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  IP: <MetalLB_IP>                                        ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    COUCHE SERVICES (HTTPS)                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   Keycloak    ‚îÇ  ‚îÇ    Vault     ‚îÇ  ‚îÇ     Kibana         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ :8443/auth    ‚îÇ  ‚îÇ   :8443/ui   ‚îÇ  ‚îÇ     :8443/         ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ                  ‚îÇ                   ‚îÇ
           ‚Üì                  ‚Üì                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  COUCHE DONN√âES (Persistence)                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  PostgreSQL   ‚îÇ  ‚îÇ  Vault Raft  ‚îÇ  ‚îÇ  Elasticsearch     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  PVC 10Gi     ‚îÇ  ‚îÇ  PVC 10Gi√ó3  ‚îÇ  ‚îÇ  StatefulSet       ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ                  ‚îÇ                   ‚îÇ
           ‚Üì                  ‚Üì                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                COUCHE STOCKAGE (Persistent Volumes)             ‚îÇ
‚îÇ  Standard StorageClass (dynamic provisioning)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Flux de Donn√©es

```
Utilisateur
   ‚Üì
DNS (keycloak.local.lab, vault.local.lab, kibana.local.lab)
   ‚Üì
/etc/hosts ‚Üí IP MetalLB
   ‚Üì
MetalLB Load Balancer
   ‚Üì
NGINX Ingress Controller
   ‚Üì
Services Kubernetes (keycloak-http, vault, kibana-kibana)
   ‚Üì
Pods Applications (Keycloak, Vault, Kibana)
   ‚Üì
Bases de Donn√©es (PostgreSQL, Elasticsearch, Vault Raft)
   ‚Üì
Persistent Volumes (PVC)
```

---

## üì¶ Installation Chronologique des Composants

### Phase 1 : Infrastructure de Base

#### √âtape 1 : Cluster Kubernetes (Kind)
**Script** : `deploy/01-cluster-kind.sh`

```bash
# Cr√©ation du cluster Kind
kind create cluster --config kind-config.yaml
```

**R√©sultat** :
- Cluster Kubernetes local multi-node
- Support pour Ingress et LoadBalancer
- Port mapping pour acc√®s externe

---

### Phase 2 : Observabilit√© (SIEM)

#### √âtape 2 : Elasticsearch
**Script** : `deploy/10-elasticsearch.sh`
**Namespace** : `security-siem`

```bash
helm install elasticsearch elastic/elasticsearch \
  --namespace security-siem \
  --set replicas=1 \
  --set minimumMasterNodes=1
```

**Composants install√©s** :
- ‚úÖ Elasticsearch StatefulSet
- ‚úÖ Service `elasticsearch-master:9200`
- ‚úÖ Secret `elasticsearch-master-credentials`

**Configuration** :
- **Replicas** : 1 (single-node pour dev)
- **S√©curit√©** : X-Pack Security activ√©
- **Credentials** : User `elastic` + password auto-g√©n√©r√©

---

#### √âtape 3 : Kibana
**Script** : `deploy/11-kibana.sh`
**Namespace** : `security-siem`

```bash
helm install kibana elastic/kibana \
  --namespace security-siem \
  --set elasticsearchHosts="http://elasticsearch-master:9200"
```

**Composants install√©s** :
- ‚úÖ Kibana Deployment
- ‚úÖ Service `kibana-kibana:5601`
- ‚úÖ Connexion √† Elasticsearch

**Interface** :
- Port : 5601
- URL : `http://localhost:5601` (via port-forward)

---

#### √âtape 4 : Filebeat
**Script** : `deploy/12-filebeat.sh`
**Namespace** : `security-siem`

```bash
helm install filebeat elastic/filebeat \
  --namespace security-siem
```

**R√¥le** : Collecte des logs Kubernetes ‚Üí Elasticsearch

---

#### √âtape 5 : Prometheus + Grafana
**Script** : `deploy/13-prometheus.sh`
**Namespace** : `security-siem`

```bash
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace security-siem
```

**Composants install√©s** :
- ‚úÖ Prometheus (m√©triques)
- ‚úÖ Grafana (visualisation)
- ‚úÖ Alertmanager (alertes)
- ‚úÖ Node Exporter
- ‚úÖ Kube-state-metrics

---

### Phase 3 : IAM et Secrets Management

#### √âtape 6 : cert-manager
**Script** : `deploy/20-cert-manager.sh`
**Namespace** : `cert-manager`

```bash
helm install cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --set installCRDs=true
```

**R√¥le** : Gestion automatique des certificats TLS (Let's Encrypt, auto-sign√©s)

---

#### √âtape 7 : Keycloak (IAM)
**Script** : `deploy/21-keycloak.sh`
**Namespace** : `security-iam`

```bash
# D√©ploiement PostgreSQL pour Keycloak
helm install keycloak-postgresql bitnami/postgresql \
  --namespace security-iam \
  --set auth.username=keycloak \
  --set auth.password=keycloak123 \
  --set auth.database=keycloak \
  --set primary.persistence.enabled=false  # ‚ö†Ô∏è Pas de persistence initialement

# D√©ploiement Keycloak
helm install keycloak codecentric/keycloak \
  --namespace security-iam \
  --set keycloak.username=admin \
  --set keycloak.password=admin123 \
  --set postgresql.enabled=false \
  --set keycloak.extraEnv="DB_VENDOR=postgres,DB_ADDR=keycloak-postgresql,..."
```

**‚ö†Ô∏è Probl√®me D√©tect√© Plus Tard** :
Keycloak utilisait en r√©alit√© **H2 embarqu√©** au lieu de PostgreSQL !
Voir [Phase 6 : Corrections et Migrations](#phase-6-corrections-et-migrations)

**Composants install√©s** :
- ‚úÖ Keycloak StatefulSet
- ‚úÖ PostgreSQL StatefulSet (non utilis√© initialement)
- ‚úÖ Services : `keycloak-http`, `keycloak-headless`

**Credentials par d√©faut** :
- Username : `admin`
- Password : `admin123`

---

#### √âtape 8 : HashiCorp Vault (Mode Dev)
**Script** : `deploy/22-vault-dev.sh`
**Namespace** : `security-iam`

```bash
helm install vault hashicorp/vault \
  --namespace security-iam \
  --set server.dev.enabled=true
```

**R√¥le** : D√©ploiement rapide en mode d√©veloppement (donn√©es en RAM)

---

#### √âtape 9 : HashiCorp Vault (Mode Raft HA)
**Script** : `deploy/23-vault-raft.sh`
**Namespace** : `security-iam`

```bash
helm install vault hashicorp/vault \
  --namespace security-iam \
  --set server.ha.enabled=true \
  --set server.ha.raft.enabled=true \
  --set server.dataStorage.enabled=true \
  --set server.dataStorage.size=10Gi
```

**Composants install√©s** :
- ‚úÖ 3 pods Vault (vault-0, vault-1, vault-2)
- ‚úÖ Mode Haute Disponibilit√© (Raft consensus)
- ‚úÖ 3 PVC 10Gi pour persistence
- ‚úÖ Services : `vault`, `vault-active`, `vault-standby`, `vault-ui`

**Initialisation** :
```bash
kubectl exec -n security-iam vault-0 -- vault operator init -format=json > vault-keys.txt
```

**Unseal** (n√©cessaire apr√®s chaque red√©marrage) :
```bash
# Unseal avec 3 cl√©s (threshold de 3 sur 5)
kubectl exec -n security-iam vault-0 -- vault operator unseal <KEY1>
kubectl exec -n security-iam vault-0 -- vault operator unseal <KEY2>
kubectl exec -n security-iam vault-0 -- vault operator unseal <KEY3>
```

---

#### √âtape 10 : Vault PKI (Certificats internes)
**Script** : `deploy/24-vault-pki.sh`
**Namespace** : `security-iam`

```bash
# Configuration PKI dans Vault
vault secrets enable pki
vault write pki/root/generate/internal \
  common_name="Enterprise Security CA" \
  ttl=87600h
```

**R√¥le** : Autorit√© de Certification (CA) interne pour mTLS

---

### Phase 4 : Runtime Security

#### √âtape 11 : Falco (eBPF Runtime Security)
**Script** : `deploy/30-falco.sh`
**Namespace** : `security-detection`

```bash
helm install falco falcosecurity/falco \
  --namespace security-detection \
  --set falco.grpc.enabled=true
```

**R√¥le** : D√©tection de comportements suspects (shell inverse, privilege escalation, etc.)

---

#### √âtape 12 : Falco Sidekick (Int√©gration Elasticsearch)
**Scripts** :
- `deploy/31-falco-sidekick.sh`
- `deploy/31-falco-elasticsearch-config.sh`

```bash
helm install falco-sidekick falcosecurity/falco-sidekick \
  --namespace security-detection \
  --set config.elasticsearch.hostport=http://elasticsearch-master.security-siem:9200
```

**R√¥le** : Envoie les alertes Falco vers Elasticsearch pour corr√©lation SIEM

---

#### √âtape 13 : Wazuh (EDR/HIDS)
**Script** : `deploy/31-wazuh.sh`
**Namespace** : `security-detection`

```bash
helm install wazuh wazuh/wazuh \
  --namespace security-detection
```

**R√¥le** : Host Intrusion Detection System (HIDS)

---

#### √âtape 14 : Int√©gration Falco + Grafana
**Scripts** :
- `deploy/32-falco-grafana.sh`
- `deploy/33-falco-dashboard-import.sh`
- `deploy/34-falco-tuning.sh`

**R√¥le** : Dashboards Grafana pour visualiser les alertes Falco

---

### Phase 5 : Policy Enforcement et Compliance

#### √âtape 15 : OPA Gatekeeper
**Script** : `deploy/40-gatekeeper.sh`
**Namespace** : `gatekeeper-system`

```bash
helm install gatekeeper gatekeeper/gatekeeper \
  --namespace gatekeeper-system
```

**R√¥le** : Policy Enforcement (PSP, admission control)

---

#### √âtape 16 : Trivy Operator
**Script** : `deploy/41-trivy.sh`
**Namespace** : `trivy-system`

```bash
helm install trivy-operator aqua/trivy-operator \
  --namespace trivy-system
```

**R√¥le** : Scan automatique des vuln√©rabilit√©s dans les images

---

#### √âtape 17 : Trivy + Grafana
**Script** : `deploy/42-trivy-grafana.sh`

**R√¥le** : Dashboards Grafana pour visualiser les vuln√©rabilit√©s d√©tect√©es

---

#### √âtape 18 : Trivy + Elasticsearch
**Script** : `deploy/43-trivy-elasticsearch.sh`

**R√¥le** : Envoie les r√©sultats de scan vers Elasticsearch

---

### Phase 6 : Networking (Ingress + Load Balancer)

#### √âtape 19 : MetalLB (Load Balancer)
**Script** : `deploy/50-metallb.sh`
**Namespace** : `metallb-system`

```bash
helm install metallb metallb/metallb \
  --namespace metallb-system
```

**Configuration** :
```yaml
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: default
  namespace: metallb-system
spec:
  addresses:
  - 172.18.255.200-172.18.255.250  # Plage d'IP pour LoadBalancer
```

**R√¥le** : Fournit des IPs externes pour les services de type LoadBalancer

---

#### √âtape 20 : NGINX Ingress Controller
**Script** : `deploy/51-nginx-ingress.sh`
**Namespace** : `ingress-nginx`

```bash
helm install ingress-nginx ingress-nginx/ingress-nginx \
  --namespace ingress-nginx \
  --set controller.service.type=LoadBalancer
```

**R√©sultat** :
- ‚úÖ Service `ingress-nginx-controller` de type LoadBalancer
- ‚úÖ IP externe assign√©e par MetalLB
- ‚úÖ Ports : 80 (HTTP) et 443 (HTTPS)

**V√©rification** :
```bash
kubectl get svc -n ingress-nginx ingress-nginx-controller
# EXTERNAL-IP: 172.18.255.200 (exemple)
```

---

#### √âtape 21 : Ingress Resources (Keycloak + Vault)
**Scripts** :
- `deploy/52-ingress-resources.sh`
- `deploy/52b-ingress-keycloak-vault.sh`

**Configuration Ingress Keycloak** :
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: keycloak-ingress
  namespace: security-iam
spec:
  ingressClassName: nginx
  rules:
  - host: keycloak.local.lab
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: keycloak-http
            port:
              number: 80
```

**Configuration Ingress Vault** :
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: vault-ingress
  namespace: security-iam
spec:
  ingressClassName: nginx
  rules:
  - host: vault.local.lab
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: vault
            port:
              number: 8200
```

**Configuration Ingress Kibana** :
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: kibana-ingress
  namespace: security-siem
spec:
  ingressClassName: nginx
  rules:
  - host: kibana.local.lab
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: kibana-kibana
            port:
              number: 5601
```

---

#### √âtape 22 : Configuration TLS
**Scripts** :
- `deploy/53-ingress-tls.sh`
- `deploy/54-cert-manager-rbac-fix.sh`
- `deploy/55-cert-manager-restart.sh`
- `deploy/56-certificates-force-retry.sh`
- `deploy/57-vault-pki-fix-cn.sh`

**R√¥le** : Certificats TLS auto-sign√©s pour les Ingress

---

## üîß Probl√®mes Rencontr√©s et Solutions

### Probl√®me 1 : Keycloak Ingress - Endpoints Vides

**Date** : Novembre 2025
**Sympt√¥me** : L'Ingress Keycloak ne fonctionnait pas, `kubectl get endpoints` montrait `<none>`

**Diagnostic** :
```bash
kubectl get endpoints -n security-iam keycloak-http
# NAME             ENDPOINTS
# keycloak-http    <none>
```

**Cause Racine** :
Les selectors du service `keycloak-http` cherchaient le label `app.kubernetes.io/instance=keycloak`, mais le pod Keycloak n'avait que `app.kubernetes.io/name=keycloak`.

**Solution** :
Script cr√©√© : `scripts/fix-keycloak-service-labels.sh`

```bash
# Ajouter le label manquant au StatefulSet
kubectl patch statefulset keycloak -n security-iam --type=merge -p '
{
  "spec": {
    "template": {
      "metadata": {
        "labels": {
          "app.kubernetes.io/instance": "keycloak"
        }
      }
    }
  }
}'

# Red√©marrer le pod pour appliquer les labels
kubectl delete pod keycloak-0 -n security-iam
```

**R√©sultat** :
‚úÖ Endpoints cr√©√©s
‚úÖ Ingress fonctionnel
‚úÖ Acc√®s via `https://keycloak.local.lab:8443/auth/admin/`

---

### Probl√®me 2 : Keycloak utilisait H2 au lieu de PostgreSQL

**Date** : Novembre 2025
**Sympt√¥me** : D√©couverte que Keycloak utilisait la base H2 embarqu√©e malgr√© PostgreSQL d√©ploy√©

**Diagnostic** :
```bash
kubectl describe pod keycloak-0 -n security-iam | grep DB_VENDOR
# DB_VENDOR: h2  ‚ùå

kubectl logs keycloak-0 -n security-iam | grep database
# databaseUrl=jdbc:h2:/opt/jboss/keycloak/standalone/data/keycloak  ‚ùå
```

**Cause Racine** :
La configuration Helm de Keycloak n'avait pas correctement appliqu√© les variables d'environnement PostgreSQL.

**Impact** :
- ‚ùå Donn√©es en H2 (non production-ready)
- ‚ùå PVC `keycloak-data-persistent` utilis√© (2Gi)
- ‚ùå PostgreSQL d√©ploy√© mais vide (pas utilis√©)
- ‚ùå User admin stock√© dans H2, pas dans PostgreSQL

**Solution** :
Migration compl√®te H2 ‚Üí PostgreSQL

Script cr√©√© : `scripts/migrate-keycloak-h2-to-postgresql.sh`

**√âtapes de la migration** :

1. **Export des donn√©es H2**
```bash
# Export via Keycloak Admin API
curl -X GET "http://localhost:8080/auth/admin/realms/master" \
  -H "Authorization: Bearer $TOKEN" > realm-master.json
```

2. **Activation de la persistence PostgreSQL**
```bash
# ‚ö†Ô∏è Probl√®me : Kubernetes interdit de modifier volumeClaimTemplates sur un StatefulSet existant
# Solution : Recr√©er le StatefulSet PostgreSQL

kubectl delete statefulset keycloak-postgresql -n security-iam --cascade=orphan
kubectl delete pod keycloak-postgresql-0 -n security-iam

helm upgrade --install keycloak-postgresql bitnami/postgresql \
  --namespace security-iam \
  --set primary.persistence.enabled=true \
  --set primary.persistence.size=10Gi
```

3. **Reconfiguration de Keycloak**
```bash
kubectl patch statefulset keycloak -n security-iam --type=json -p='[
  {
    "op": "replace",
    "path": "/spec/template/spec/containers/0/env",
    "value": [
      {"name": "DB_VENDOR", "value": "postgres"},
      {"name": "DB_ADDR", "value": "keycloak-postgresql"},
      {"name": "DB_PORT", "value": "5432"},
      {"name": "DB_DATABASE", "value": "keycloak"},
      {"name": "DB_USER", "value": "keycloak"},
      {"name": "DB_PASSWORD", "value": "keycloak123"}
    ]
  }
]'

kubectl delete pod keycloak-0 -n security-iam
```

4. **V√©rification**
```bash
kubectl logs keycloak-0 -n security-iam | grep database
# databaseUrl=jdbc:postgresql://keycloak-postgresql:5432/keycloak  ‚úÖ
# databaseProduct=PostgreSQL 18.1  ‚úÖ
```

**R√©sultat** :
- ‚úÖ Keycloak utilise maintenant PostgreSQL
- ‚úÖ PVC PostgreSQL 10Gi cr√©√© (`data-keycloak-postgresql-0`)
- ‚úÖ Donn√©es persistantes et production-ready
- ‚úÖ Admin user automatiquement recr√©√© par Keycloak

**Documentation** :
- `docs/H2-TO-POSTGRESQL-MIGRATION.md`
- `docs/PERSISTENCE-ARCHITECTURE.md`

---

### Probl√®me 3 : Kibana - Authentification √âchou√©e

**Date** : Novembre 2025
**Sympt√¥me** : Impossible de se connecter √† Kibana avec les credentials d√©cod√©s du secret

**Diagnostic** :
```bash
# R√©cup√©ration des credentials du secret
kubectl get secret elasticsearch-master-credentials -n security-siem -o jsonpath='{.data.username}' | base64 -d
# elastic

kubectl get secret elasticsearch-master-credentials -n security-siem -o jsonpath='{.data.password}' | base64 -d
# 3Yk13LXAaWntSAHRv

# Test d'authentification
kubectl exec -n security-siem elasticsearch-master-0 -- \
  curl -k -u elastic:3Yk13LXAaWntSAHRv https://localhost:9200/_cluster/health

# {"error":{"type":"security_exception","reason":"unable to authenticate user [elastic]"},"status":401}  ‚ùå
```

**Cause Racine** :
Le mot de passe dans le secret Kubernetes ne correspondait **pas** au mot de passe r√©ellement configur√© dans Elasticsearch.

**Solution** :
R√©initialisation du mot de passe Elasticsearch + synchronisation du secret

Script cr√©√© : `scripts/fix-kibana-auth.sh`

```bash
# R√©initialiser le mot de passe 'elastic'
kubectl exec -n security-siem elasticsearch-master-0 -- \
  /usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic -b

# R√©cup√©rer le nouveau password
NEW_PASSWORD=<password_affich√©>

# Mettre √† jour le secret Kubernetes
kubectl create secret generic elasticsearch-master-credentials \
  --from-literal=username=elastic \
  --from-literal=password=$NEW_PASSWORD \
  --namespace security-siem \
  --dry-run=client -o yaml | kubectl apply -f -

# Red√©marrer Kibana
kubectl rollout restart deployment/kibana-kibana -n security-siem
```

**R√©sultat** :
‚úÖ Authentification Elasticsearch fonctionnelle
‚úÖ Kibana accessible via `https://kibana.local.lab:8443/`
‚úÖ Nouveaux credentials synchronis√©s

---

## üåê Configuration R√©seau et Ingress

### Architecture R√©seau

```
Internet / Utilisateur
         ‚Üì
/etc/hosts (DNS local)
  keycloak.local.lab ‚Üí 172.18.255.200
  vault.local.lab    ‚Üí 172.18.255.200
  kibana.local.lab   ‚Üí 172.18.255.200
         ‚Üì
MetalLB Load Balancer (172.18.255.200)
         ‚Üì
NGINX Ingress Controller (ingress-nginx-controller)
         ‚Üì
         ‚îú‚îÄ‚Üí Host: keycloak.local.lab ‚Üí Service: keycloak-http:80  ‚Üí Pod: keycloak-0
         ‚îú‚îÄ‚Üí Host: vault.local.lab    ‚Üí Service: vault:8200        ‚Üí Pods: vault-{0,1,2}
         ‚îî‚îÄ‚Üí Host: kibana.local.lab   ‚Üí Service: kibana-kibana:5601‚Üí Pod: kibana-*
```

### Configuration `/etc/hosts`

Pour acc√©der aux services via Ingress, ajoutez ces lignes dans `/etc/hosts` :

```bash
# R√©cup√©rer l'IP MetalLB
INGRESS_IP=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].ip}')

# Ajouter dans /etc/hosts
echo "$INGRESS_IP keycloak.local.lab vault.local.lab kibana.local.lab" | sudo tee -a /etc/hosts
```

Exemple :
```
172.18.255.200 keycloak.local.lab vault.local.lab kibana.local.lab
```

### Ports et Protocoles

| Service | Port Interne | Port Ingress | Protocole |
|---------|--------------|--------------|-----------|
| Keycloak | 8080 | 8443 | HTTPS |
| Vault | 8200 | 8443 | HTTPS |
| Kibana | 5601 | 8443 | HTTPS |
| Elasticsearch | 9200 | - | HTTP (interne) |
| PostgreSQL | 5432 | - | TCP (interne) |
| Prometheus | 9090 | - | HTTP (interne) |
| Grafana | 3000 | - | HTTP (interne) |

### Services Kubernetes

```bash
# Namespace: security-iam
kubectl get svc -n security-iam
NAME                       TYPE        CLUSTER-IP       PORT(S)
keycloak-headless          ClusterIP   None             80/TCP
keycloak-http              ClusterIP   10.110.179.3     80/TCP,8443/TCP,9990/TCP
keycloak-postgresql        ClusterIP   10.99.244.42     5432/TCP
vault                      ClusterIP   10.99.132.254    8200/TCP,8201/TCP
vault-active               ClusterIP   10.100.26.242    8200/TCP,8201/TCP
vault-standby              ClusterIP   10.101.111.235   8200/TCP,8201/TCP
vault-ui                   ClusterIP   10.106.239.186   8200/TCP

# Namespace: security-siem
kubectl get svc -n security-siem
NAME                      TYPE        CLUSTER-IP       PORT(S)
elasticsearch-master      ClusterIP   10.96.128.45     9200/TCP,9300/TCP
kibana-kibana             ClusterIP   10.98.54.123     5601/TCP
prometheus-kube-state     ClusterIP   10.105.23.67     8080/TCP
prometheus-grafana        ClusterIP   10.107.89.234    80/TCP

# Namespace: ingress-nginx
kubectl get svc -n ingress-nginx
NAME                       TYPE           EXTERNAL-IP      PORT(S)
ingress-nginx-controller   LoadBalancer   172.18.255.200   80:30080/TCP,443:30443/TCP
```

---

## üìä √âtat Actuel du Projet

### Composants D√©ploy√©s (‚úÖ = Op√©rationnel)

| Composant | Namespace | Pods | Status | Acc√®s |
|-----------|-----------|------|--------|-------|
| **Keycloak** | security-iam | 1 | ‚úÖ Running | https://keycloak.local.lab:8443/auth/admin/ |
| **PostgreSQL (Keycloak)** | security-iam | 1 | ‚úÖ Running | Interne (PVC 10Gi) |
| **Vault** | security-iam | 3 | ‚úÖ Running (HA) | https://vault.local.lab:8443/ui/ |
| **Elasticsearch** | security-siem | 1 | ‚úÖ Running | Interne (9200) |
| **Kibana** | security-siem | 1 | ‚úÖ Running | https://kibana.local.lab:8443/ |
| **Prometheus** | security-siem | 1 | ‚úÖ Running | Port-forward 9090 |
| **Grafana** | security-siem | 1 | ‚úÖ Running | Port-forward 3000 |
| **Falco** | security-detection | 1 | ‚úÖ Running | - |
| **Wazuh** | security-detection | - | ‚ö†Ô∏è D√©ploy√© | - |
| **Trivy Operator** | trivy-system | 1 | ‚úÖ Running | - |
| **Gatekeeper** | gatekeeper-system | 3 | ‚úÖ Running | - |
| **MetalLB** | metallb-system | 2 | ‚úÖ Running | - |
| **NGINX Ingress** | ingress-nginx | 1 | ‚úÖ Running | 172.18.255.200 |

### Persistent Volumes (PVC)

```bash
kubectl get pvc --all-namespaces

NAMESPACE       NAME                         STATUS   VOLUME     CAPACITY   STORAGECLASS
security-iam    data-keycloak-postgresql-0   Bound    pvc-001    10Gi       standard
security-iam    data-vault-0                 Bound    pvc-002    10Gi       standard
security-iam    data-vault-1                 Bound    pvc-003    10Gi       standard
security-iam    data-vault-2                 Bound    pvc-004    10Gi       standard
security-iam    keycloak-data-persistent     Bound    pvc-005    2Gi        standard  # ‚ö†Ô∏è Ancien (H2), peut √™tre supprim√©
```

**Total Stockage Utilis√©** : 42 Gi

### Ingress Configur√©s

```bash
kubectl get ingress --all-namespaces

NAMESPACE       NAME               HOSTS                  ADDRESS          PORTS
security-iam    keycloak-ingress   keycloak.local.lab     172.18.255.200   80, 443
security-iam    vault-ingress      vault.local.lab        172.18.255.200   80, 443
security-siem   kibana-ingress     kibana.local.lab       172.18.255.200   80, 443
```

---

## üîê Acc√®s et Credentials

### Services Accessibles via Ingress

| Service | URL | Username | Password | Notes |
|---------|-----|----------|----------|-------|
| **Keycloak** | https://keycloak.local.lab:8443/auth/admin/ | admin | admin123 | IAM / SSO |
| **Vault** | https://vault.local.lab:8443/ui/ | - | Voir vault-keys.txt | Root Token requis |
| **Kibana** | https://kibana.local.lab:8443/ | elastic | <nouveau_password> | SIEM |

### Services Accessibles via Port-Forward

```bash
# Grafana
kubectl port-forward -n security-siem svc/prometheus-grafana 3000:80
# http://localhost:3000
# Username: admin
# Password: prom-operator (par d√©faut)

# Prometheus
kubectl port-forward -n security-siem svc/prometheus-kube-prometheus-prometheus 9090:9090
# http://localhost:9090

# Elasticsearch (direct)
kubectl port-forward -n security-siem svc/elasticsearch-master 9200:9200
# http://localhost:9200

# PostgreSQL (Keycloak)
kubectl port-forward -n security-iam svc/keycloak-postgresql 5432:5432
# psql -h localhost -U keycloak -d keycloak
# Password: keycloak123
```

### Secrets Kubernetes Importants

```bash
# Credentials Elasticsearch
kubectl get secret elasticsearch-master-credentials -n security-siem -o jsonpath='{.data.username}' | base64 -d
kubectl get secret elasticsearch-master-credentials -n security-siem -o jsonpath='{.data.password}' | base64 -d

# Credentials PostgreSQL (Keycloak)
kubectl get secret keycloak-postgresql -n security-iam -o jsonpath='{.data.password}' | base64 -d

# Vault Root Token et Unseal Keys
cat vault-keys.txt  # Fichier cr√©√© lors de l'init Vault
```

### Vault - Unseal Process

Vault n√©cessite un **unseal** apr√®s chaque red√©marrage :

```bash
# V√©rifier le statut
kubectl exec -n security-iam vault-0 -- vault status
# Sealed: true  ‚Üê N√©cessite unseal

# Unseal avec 3 cl√©s (threshold)
kubectl exec -n security-iam vault-0 -- vault operator unseal <UNSEAL_KEY_1>
kubectl exec -n security-iam vault-0 -- vault operator unseal <UNSEAL_KEY_2>
kubectl exec -n security-iam vault-0 -- vault operator unseal <UNSEAL_KEY_3>

# V√©rifier
kubectl exec -n security-iam vault-0 -- vault status
# Sealed: false  ‚úÖ
```

---

## üõ†Ô∏è Scripts et Outils Cr√©√©s

### Scripts de D√©ploiement (deploy/)

| Script | Description |
|--------|-------------|
| `01-cluster-kind.sh` | Cr√©ation du cluster Kind |
| `10-elasticsearch.sh` | D√©ploiement Elasticsearch |
| `11-kibana.sh` | D√©ploiement Kibana |
| `12-filebeat.sh` | D√©ploiement Filebeat |
| `13-prometheus.sh` | D√©ploiement Prometheus + Grafana |
| `20-cert-manager.sh` | D√©ploiement cert-manager |
| `21-keycloak.sh` | D√©ploiement Keycloak + PostgreSQL |
| `22-vault-dev.sh` | D√©ploiement Vault (mode dev) |
| `23-vault-raft.sh` | D√©ploiement Vault (HA Raft) |
| `24-vault-pki.sh` | Configuration Vault PKI |
| `30-falco.sh` | D√©ploiement Falco |
| `31-falco-sidekick.sh` | D√©ploiement Falco Sidekick |
| `40-gatekeeper.sh` | D√©ploiement OPA Gatekeeper |
| `41-trivy.sh` | D√©ploiement Trivy Operator |
| `50-metallb.sh` | D√©ploiement MetalLB |
| `51-nginx-ingress.sh` | D√©ploiement NGINX Ingress |
| `52-ingress-resources.sh` | Cr√©ation des Ingress |
| `53-ingress-tls.sh` | Configuration TLS |

### Scripts de Correction (scripts/)

| Script | Description | Cr√©√© le |
|--------|-------------|---------|
| `fix-keycloak-service-labels.sh` | Correction labels Keycloak pour Ingress | Nov 2025 |
| `migrate-keycloak-h2-to-postgresql.sh` | Migration H2 ‚Üí PostgreSQL | Nov 2025 |
| `enable-postgresql-persistence-safe.sh` | Activation persistence PostgreSQL (avec backup) | Nov 2025 |
| `fix-kibana-auth.sh` | Correction authentification Kibana | Nov 2025 |
| `verify-stack-health.sh` | V√©rification compl√®te de la stack | Nov 2025 |

### Scripts de V√©rification

#### `verify-stack-health.sh`

Script complet de v√©rification de l'√©tat de la stack :

```bash
./scripts/verify-stack-health.sh
```

**V√©rifie** :
- ‚úÖ Pods Keycloak, Vault, PostgreSQL, Elasticsearch, Kibana
- ‚úÖ Services et Endpoints
- ‚úÖ Ingress et IP MetalLB
- ‚úÖ Statut Vault (sealed/unsealed)
- ‚úÖ Connexion PostgreSQL
- ‚úÖ Tables Keycloak dans PostgreSQL

**Affiche** :
- √âtat des composants
- URLs d'acc√®s
- Credentials par d√©faut
- PVC cr√©√©s

---

## üìö Documentation Cr√©√©e

| Document | Description |
|----------|-------------|
| `README.md` | Documentation principale du projet |
| `docs/PERSISTENCE-ARCHITECTURE.md` | Architecture de persistence PostgreSQL |
| `docs/H2-TO-POSTGRESQL-MIGRATION.md` | Guide de migration H2 ‚Üí PostgreSQL |
| `CREDENTIALS.md` | Liste des credentials de tous les services |
| `TROUBLESHOOTING.md` | Guide de d√©pannage |
| `KEYCLOAK-INGRESS-SETUP.md` | Configuration Ingress Keycloak |
| `KIBANA-CLEANUP.md` | Proc√©dures de nettoyage Kibana |
| `PORT-FORWARD-GUIDE.md` | Guide port-forward pour tous les services |

---

## üéØ Prochaines √âtapes Recommand√©es

### S√©curit√©

- [ ] Rotation des credentials par d√©faut (admin/admin123)
- [ ] Configuration Let's Encrypt pour certificats TLS production
- [ ] Activation MFA sur Keycloak
- [ ] Configuration RBAC Kubernetes granulaire
- [ ] Scan de s√©curit√© avec Trivy sur tous les pods

### Haute Disponibilit√©

- [ ] R√©plication PostgreSQL (primary + replica)
- [ ] Scaling horizontal Keycloak (2-3 pods)
- [ ] Scaling Elasticsearch (3 nodes cluster)
- [ ] Configuration PodDisruptionBudget
- [ ] Backup automatique des PVC

### Monitoring

- [ ] Configuration Alertmanager (alertes Slack/Email)
- [ ] Dashboards Grafana personnalis√©s
- [ ] Corr√©lation logs Falco + Elasticsearch
- [ ] M√©triques custom Prometheus
- [ ] Tracing distribu√© (Jaeger/Tempo)

### Int√©grations

- [ ] SSO Keycloak pour Kibana
- [ ] SSO Keycloak pour Grafana
- [ ] SSO Keycloak pour Vault
- [ ] Integration Vault ‚Üí Kubernetes (secrets injection)
- [ ] ArgoCD pour GitOps

### Compliance

- [ ] Scan CIS Benchmarks avec Kube-bench
- [ ] Audit Prowler (si cloud public)
- [ ] Policy OPA Gatekeeper (require labels, image scanning, etc.)
- [ ] G√©n√©ration de rapports de conformit√©

---

## üìä M√©triques et KPI

### Ressources Cluster

```bash
# Utilisation CPU/RAM par namespace
kubectl top nodes
kubectl top pods --all-namespaces

# Nombre de pods par namespace
kubectl get pods --all-namespaces --no-headers | awk '{print $1}' | sort | uniq -c

# Stockage total utilis√©
kubectl get pvc --all-namespaces -o json | jq '.items[].spec.resources.requests.storage' | awk '{sum+=$1} END {print sum " Gi"}'
```

### Disponibilit√©

| Service | Uptime Cible | R√©plication |
|---------|--------------|-------------|
| Keycloak | 99.9% | 1 pod (√† scaler) |
| Vault | 99.9% | 3 pods (HA Raft) |
| Elasticsearch | 99% | 1 node (√† scaler) |
| Kibana | 99% | 1 pod |
| PostgreSQL | 99.9% | 1 pod (√† r√©pliquer) |

---

## üîÑ Changelog du Projet

### Novembre 2025

**15/11/2025** :
- ‚úÖ Migration Keycloak : H2 ‚Üí PostgreSQL
- ‚úÖ Activation persistence PostgreSQL (10Gi PVC)
- ‚úÖ Correction Ingress Keycloak (labels manquants)
- ‚úÖ Correction authentification Kibana
- ‚úÖ Cr√©ation script `verify-stack-health.sh`
- ‚úÖ Documentation compl√®te de la migration
- ‚úÖ Tous les services accessibles via Ingress

**12/11/2025** :
- ‚úÖ D√©ploiement Ingress Keycloak, Vault, Kibana
- ‚úÖ Configuration MetalLB + NGINX Ingress
- ‚ö†Ô∏è Probl√®me endpoints Keycloak d√©tect√©

**10/11/2025** :
- ‚úÖ D√©ploiement initial ELK Stack
- ‚úÖ D√©ploiement Prometheus + Grafana
- ‚úÖ D√©ploiement Keycloak + PostgreSQL (H2 utilis√© par erreur)
- ‚úÖ D√©ploiement Vault HA (Raft)

**Avant** :
- ‚úÖ Cr√©ation du cluster Kind
- ‚úÖ D√©ploiement Falco, Trivy, Gatekeeper
- ‚úÖ Configuration initiale des namespaces

---

## üèÜ Conclusion

### Points Forts du Projet

‚úÖ **Architecture Production-Ready** :
- Keycloak + PostgreSQL avec persistence
- Vault en mode Haute Disponibilit√© (3 nodes Raft)
- ELK Stack pour SIEM
- Ingress + MetalLB pour acc√®s externe

‚úÖ **S√©curit√© Multi-Couches** :
- IAM (Keycloak)
- Secrets Management (Vault)
- Runtime Security (Falco)
- Vulnerability Scanning (Trivy)
- Policy Enforcement (Gatekeeper)

‚úÖ **Observabilit√© Compl√®te** :
- Logs centralis√©s (ELK)
- M√©triques (Prometheus)
- Visualisation (Grafana + Kibana)

‚úÖ **Infrastructure as Code** :
- Scripts Bash automatis√©s
- Configuration Helm
- Kubernetes manifests
- Documentation compl√®te

### √âquivalence Commerciale D√©montr√©e

| Solution Commercial | Impl√©mentation Open-Source | Status |
|---------------------|---------------------------|--------|
| Okta | Keycloak | ‚úÖ Op√©rationnel |
| CyberArk / AWS Secrets Manager | HashiCorp Vault | ‚úÖ Op√©rationnel |
| Splunk / QRadar | ELK Stack | ‚úÖ Op√©rationnel |
| CrowdStrike | Falco | ‚úÖ Op√©rationnel |
| Aqua / Prisma Cloud | Trivy + Gatekeeper | ‚úÖ Op√©rationnel |

---

## üìû Support et Ressources

### Documentation Officielle

- **Keycloak** : https://www.keycloak.org/documentation
- **HashiCorp Vault** : https://developer.hashicorp.com/vault/docs
- **Elasticsearch** : https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html
- **Kibana** : https://www.elastic.co/guide/en/kibana/current/index.html
- **Falco** : https://falco.org/docs/
- **Kubernetes** : https://kubernetes.io/docs/

### Commandes Utiles

```bash
# V√©rification globale
./scripts/verify-stack-health.sh

# √âtat des pods
kubectl get pods --all-namespaces

# Logs d'un service
kubectl logs -n security-iam keycloak-0 --tail=100

# Acc√®s shell √† un pod
kubectl exec -it -n security-iam keycloak-0 -- /bin/bash

# Red√©marrer un deployment
kubectl rollout restart deployment/kibana-kibana -n security-siem

# V√©rifier les Ingress
kubectl get ingress --all-namespaces

# V√©rifier les PVC
kubectl get pvc --all-namespaces
```

---

**Document cr√©√© le** : 15 Novembre 2025
**Derni√®re mise √† jour** : 15 Novembre 2025
**Version** : 1.0
**Auteur** : Z3ROX

---

> üí° **Ce document doit √™tre maintenu √† jour** avec chaque modification importante du projet. Ajoutez une entr√©e dans le Changelog √† chaque d√©ploiement majeur.
